# Toy Problem 2 with analytical mics

# ConvoNet model

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import datasets, layers, models
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import cmath as cm
import math
import random
import sys


np.set_printoptions(threshold = sys.maxsize)
np.set_printoptions(suppress=True)

sources = 1
microphones = 4
classes = 20
class_size = 9
training_samples = 50000
val_samples = 10000
pred_samples = 1
frequency = 100
no_of_epochs = 10
bs = 64

# Microphone array (5 microphone array)

x_mic = np.array([0, 0, -1, 1])
y_mic = np.array([0, math.sqrt(3)/2, -math.sqrt(3)/2, -math.sqrt(3)/2])

# Generating training data (cross-spectral-matrix and ground-truth-matrix)

def data_gen(x_mic,y_mic,sources,microphones):

 r = 10
#  r = random.randint(10,20)
 theta = np.random.uniform(0,180)  # Direction
 label = theta//class_size        # 0-8 labels
 x_source = r*np.cos(np.pi*theta/180)
 y_source = r*np.sin(np.pi*theta/180)
 if (theta>90):
   x_source = -r*np.cos(np.pi*(180-theta)/180)
   y_source = r*np.sin(np.pi*(180-theta)/180)

 p=np.empty(microphones,dtype=np.complex_)

 for i in range(microphones):
  rs = ((x_source-x_mic[i])**2 + (y_source-y_mic[i])**2)**0.5
  p[i] = np.sum(np.exp( (-2*1j*math.pi*frequency*rs)/343)/(4*math.pi*rs))
 p_H = np.conj(p)
 sample = np.dot(p[:,None],p_H[None,:])
 #sample = np.real(sample)
 sample = np.array(sample.reshape(microphones,microphones,1))
 return sample, label


training_data = np.empty((training_samples,microphones,microphones,1),dtype=complex)
training_labels = np.empty((training_samples,1), dtype=int)

validation_data = np.empty((val_samples,microphones,microphones,1),dtype=complex)
validation_labels = np.empty((val_samples,1), dtype=int)

pred_data = np.empty((pred_samples,microphones,microphones,1),dtype=complex)
pred_labels = np.empty((pred_samples,1), dtype=int)

# Functions for training, validation and prediction data

def training_data_gen():
 for i in range(training_samples):
  training_data[i],training_labels[i] = data_gen(x_mic,y_mic,sources,microphones)
 return training_data,training_labels

def val_data_gen():
 for i in range(val_samples):
  validation_data[i],validation_labels[i] = data_gen(x_mic,y_mic,sources,microphones)
 return validation_data,validation_labels

def pred_data_gen():
 for i in range(pred_samples):
  pred_data[i],pred_labels[i] = data_gen(x_mic,y_mic,sources,microphones)
 return pred_data,pred_labels

#################################

ut = training_data_gen()
uv = val_data_gen()

###############################

# CNN

model = models.Sequential()
model.add(layers.Conv2D(128, (2,2), activation='relu', input_shape=(microphones,microphones,1)))
model.add(layers.Conv2D(64, (2, 2), activation='relu'))
model.add(layers.Flatten())
model.add(layers.Dense(128, activation='relu'))
model.add(layers.Dense(classes, activation='softmax'))


opt = keras.optimizers.Adam(learning_rate=0.0001)
model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics='accuracy')
model.summary()

######################################

model.fit(ut[0],ut[1], validation_data = (uv[0],uv[1]), batch_size = 64, epochs = 200)

#########################

# Prediction New

up = pred_data_sample_gen()
z1 = model.predict(up[0])
print(z1)
print(np.argmax(z1))
k1 = np.array(up[1])
print(k1)
print(up[2])
